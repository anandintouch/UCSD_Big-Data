Video 1

Tabular data set

logs

--RDD

students = sc.parallelize ([
[100, "Alice", 8.5, "Computer Science"],
[101, "Bob", 7.1, "Engineering"],
[102, "Carl", 6.2, "Engineering"]
])

def extract_grade(row):
return row[2]

students.map(extract_grade).mean()

--Max by column

def extract_degree_grade(row):

return(row[3], row[2]

degree_grade_RDD = students.map(extract_degree_grade)

degree_grade_RDD.collect()

degree_grade_RDD.reduceByKey(max).collect()

--Spark dataframe

students_df = sqlCtx.createDataFrame(students, ["id", "name", "grade", "degree"])\

students_df.printSchema()

students_df.agg({"grade":"mean"}).collect()

students_df.groupBy("degree").max("grade").collect()

students_df.groupBy("degree").max("grade").show()

Video 2

from pyspark.sql.types import *

schema = StructType([

StructField("id", LongType(), True),

StructField("name", StringType(), True),

StructField("grade", DoubleType(), True),

StructField("degree", StringType(), True)])

students_df = sqlCtx.createDataFrame(students, schema)

students_df.printSchema()

--Load a JSON file

students_json = [

'{"id":100, "name":"Alice", "grade":8.5, "degree":"Computer Science"}',

'{"id":101, "name":"Bob", "grade":7.1, "degree":"Engineering"}']

with open("students.json", "w") as f: f.write("\n".join(students_json))

--Dump

cat students.json

--Create a DataFrame with Json file

sqlCtx.jsonFile("file:///home/cloudera/students.json").show()

--CSV

--Restart pyspark

PYSPARK_DRIVER_PYTHON=ipython pyspark --packages com.databricks:spark-csv_2.10:1.3.0

yelp_df = sqlCtx.load(

source="com.databricks.spark.csv",

header = 'true',

inferSchema = 'true',

path = 'file:///usr/lib/hue/apps/search/examples/collections/solr_configs_yelp_demo/index_data.csv')

PYSPARK_DRIVER_PYTHON=ipython pyspark --packages com.databricks:spark-csv_2.10:1.3.0

from pyspark.sql.types import *

yelp_df = sqlCtx.load(

source="com.databricks.spark.csv",

header = 'true',

inferSchema = 'true',

path = 'file:///usr/lib/hue/apps/search/examples/collections/solr_configs_yelp_demo/index_data.csv')

yelp_df.printSchema()

yelp_df.count()

-------------------------------------------------

yelp_df.filter(yelp_df.useful >=1).count()

yelp_df.filter(yelp_df["useful"] >=1).count()

yelp_df.filter("useful >=1").count()

yelp_df.select("business_id:", "useful")

yelp_df.select("business_id", "useful").agg({"useful":"max"}).collect()

yelp_df.select("id", "useful").take(5)

yelp_df.select("id", yelp_df.useful/28*100).show(5)

yelp_df.select("id", (yelp_df.useful/28*100).cast("int")).show(5)

useful_perc_data = yelp_df.select(

yelp_df.id.alias("uid"),

(yelp_df.useful/28*100).cast("int").alias("useful_perc")

)

from pyspark.sql.functions import asc, desc

useful_perc_data = yelp_df.select(

yelp_df.id.alias("uid"),

(yelp_df.useful/28*100).cast("int").alias("useful_perc")

).orderBy(desc("useful_perc"));

useful_perc_data.show(10)

--inner join

useful_perc_data2 = useful_perc_data.join(

yelp_df,

yelp_df.id == useful_perc_data.uid,

"inner")

useful_perc_data2.count()

useful_perc_data2.select("uid","useful_perc","review_count").show(10)

--Cache in memory

useful_perc_data.join(

yelp_df,

yelp_df.id == useful_perc_data.uid,

"inner").cache().select("uid","useful_perc","review_count").show(10)

SQL
===============================================================

yelp_df.registerTempTable("yelp")

filtered_yelp = sqlContext.sql("""SELECT *

FROM yelp

WHERE useful >= 1""")

useful_perc_data.registerTempTable("useful_perc_data")

joined_yelp = sqlContext.sql("""

SELECT upd.uid,

upd.useful_perc,

y.review_count

FROM useful_perc_data upd

INNER JOIN yelp y

ON upd.uid = y.id

ORDER BY y.review_count DESC""")

